{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Review Helpfulness Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we will try to automatically predict the helpfulness of Amazon.com product reviews, specifically in\n",
    "the case of home and kitchen products.\n",
    "\n",
    "The inspiration behind this project is that the product we buy is mostly influenced by the review about the product. In most of the e-commerce platform, the most helpful reviews are displayed on the products front page.In order to differentiate reviews based on their helpfulness, Amazon has implemented an interface that allows customers to vote on whether a particular review has been helpful or unhelpful. The fraction of customers who have deemed the review helpful is displayed with the review, and  amazon uses these ratings to rank the reviews, displaying the most helpful rankings on the products front page. The drawback is that more recently written reviews are at a disadvantage since less people have voted on the helpfulness of the review. Because of this, reviews with few votes cannot be effectively ranked and will not gain visibility until they have accumulated adequate votes, which can take some time.\n",
    "\n",
    "As a result, we would like to assess the helpfulness of reviews automatically, without having to wait for users to manually vote over the course of time. If we can do this, we would be able to give users the most relevant, helpful, and up to date reviews possible, without any delay in more helpful reviews being displayed. Moreover, such an automatic classification of reviews would be able to help in rooting out poorly written reviews lacking helpful information to other consumers.\n",
    "\n",
    "For the problem statement, we will use the Home and Kitchen dataset which is having around 346,355 reviews. Dataset is available at JmCauley page: http://jmcauley.ucsd.edu/data/amazon/links.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the relevant dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import math, time, random, datetime\n",
    "import string\n",
    "import os\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "from collections import defaultdict\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# data visualization\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "from IPython.display import display # Allows the use of display() for DataFrames\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import re\n",
    "from collections import Counter\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder where input data and pickle data load present\n",
    "data_folder = '../data/dataLoad/'\n",
    "input_folder = '../data/raw/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the home and kitchen dataset which is downloaded in the /data path\n",
    "input_data = 'reviews_Home_and_Kitchen_5.json.gz'\n",
    "def parse(path):\n",
    "    g = gzip.open(path, 'rb')\n",
    "    for l in g:\n",
    "        yield eval(l)\n",
    "\n",
    "def getDF(path):\n",
    "    i = 0\n",
    "    df = {}\n",
    "    for d in parse(path):\n",
    "        df[i] = d\n",
    "        i +=1\n",
    "    return pd.DataFrame.from_dict(df, orient='index')\n",
    "\n",
    "data = getDF(input_folder + input_data)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>helpful</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>APYOBQE6M18AA</td>\n",
       "      <td>0615391206</td>\n",
       "      <td>Martin Schwartz</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>My daughter wanted this book and the price on ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Best Price</td>\n",
       "      <td>1382140800</td>\n",
       "      <td>10 19, 2013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      reviewerID        asin     reviewerName helpful  \\\n",
       "0  APYOBQE6M18AA  0615391206  Martin Schwartz  [0, 0]   \n",
       "\n",
       "                                          reviewText  overall     summary  \\\n",
       "0  My daughter wanted this book and the price on ...      5.0  Best Price   \n",
       "\n",
       "   unixReviewTime   reviewTime  \n",
       "0      1382140800  10 19, 2013  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in the jmcauley site, belaw are the description of the fields:\n",
    "1. reviewerID - ID of the reviewer, e.g. A2SUAM1J3GNN3B\n",
    "2. asin - ID of the product, e.g. 0000013714\n",
    "3. reviewerName - name of the reviewer\n",
    "4. helpful - helpfulness rating of the review, e.g. 2/3.\n",
    "5. reviewText - text of the review\n",
    "6. overall - rating of the product\n",
    "7. summary - summary of the review\n",
    "8. unixReviewTime - time of the review (unix time)\n",
    "9. reviewTime - time of the review (raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before selecting features from the data, need to narrow down search to those reviews that had more than 10 total reviews. The ratio of people that find it helpful is more important. This effect can disappear at small ratings (e.g 1/2) so the reviews with less than 10 ratings have been eliminated. Beyond that we also limited our search to reviews for products had at least 10 reviews, because automated helpfulness classification itself is not necessary if the given product for which the reviews are being displayed has few reviews to begin with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Filtering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(551682, 9)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Size of the data before filtering\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(462551, 9)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter the products which has atleast 10 reviews\n",
    "groupby_product = data.groupby('asin').size()\n",
    "filter_reviews = data.groupby('asin').size().values >= 10\n",
    "filter_asins = groupby_product[filter_reviews].index\n",
    "data_filter = data.loc[data['asin'].isin(filter_asins), :]\n",
    "data_filter.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27477, 9)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter the reviews having more than 10 helpful reviews\n",
    "dataset = data_filter[data_filter['helpful'].apply(lambda x: x[1] > 10)]\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our data is reduced down to 86914 product reviews. Next is deriving the helpfulness target variable.The problem statement is the classification problem that needs to be mapped to discrete class, in our case its binary classification with 2 discrete class i.e; 1/0 which map to Helpful/Not Helpful class repectively. We use the sigmoid function in order to map predicted values to probability values. This sigmoid function then maps any real value into a probability value between 0 and 1.\n",
    "The sigmoid function returns a probability value between 0 and 1. This probability value is then mapped to a discrete class which is either “0” or “1”. In order to map this probability value to a discrete class (pass/fail, yes/no, true/false), we select a threshold value. This threshold value is called Decision boundary. Above this threshold value, we will map the probability values into class 1 and below which we will map values into class 0.\n",
    "\n",
    "Mathematically, it can be expressed as follows:-\n",
    "\n",
    "                p ≥ 0.5 => class = 1\n",
    "\n",
    "                p < 0.5 => class = 0 \n",
    "\n",
    "\n",
    "Generally, the decision boundary is set to 0.5. So, if the probability value is 0.8 (> 0.5), we will map this observation to class 1. Similarly, if the probability value is 0.2 (< 0.5), we will map this observation to class 0.\n",
    "\n",
    "We will also build custom features which will help in predicting the helpfulness further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deriving Helpfulness Target variable from features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>helpful</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>helpful_ratings</th>\n",
       "      <th>total_ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A3UPYGJKZ0XTU4</td>\n",
       "      <td>0615391206</td>\n",
       "      <td>mirasreviews</td>\n",
       "      <td>[26, 27]</td>\n",
       "      <td>There is no shortage of pop recipes available ...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Excels at Sweet Dessert Pops, but Falls Short ...</td>\n",
       "      <td>1367712000</td>\n",
       "      <td>05 5, 2013</td>\n",
       "      <td>26</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewerID        asin  reviewerName   helpful  \\\n",
       "2  A3UPYGJKZ0XTU4  0615391206  mirasreviews  [26, 27]   \n",
       "\n",
       "                                          reviewText  overall  \\\n",
       "2  There is no shortage of pop recipes available ...      4.0   \n",
       "\n",
       "                                             summary  unixReviewTime  \\\n",
       "2  Excels at Sweet Dessert Pops, but Falls Short ...      1367712000   \n",
       "\n",
       "   reviewTime  helpful_ratings  total_ratings  \n",
       "2  05 5, 2013               26             27  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['helpful_ratings'] = dataset['helpful'].apply(lambda x: x[0])\n",
    "dataset['total_ratings'] = dataset['helpful'].apply(lambda x: x[1])\n",
    "\n",
    "dataset.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>helpful_ratings</th>\n",
       "      <th>total_ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>27477.000000</td>\n",
       "      <td>2.747700e+04</td>\n",
       "      <td>27477.000000</td>\n",
       "      <td>27477.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.853769</td>\n",
       "      <td>1.261137e+09</td>\n",
       "      <td>47.941988</td>\n",
       "      <td>52.304982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.447439</td>\n",
       "      <td>9.132063e+07</td>\n",
       "      <td>339.326083</td>\n",
       "      <td>344.390836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.572256e+08</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.201651e+09</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.282090e+09</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>22.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.331251e+09</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>43.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.405642e+09</td>\n",
       "      <td>52176.000000</td>\n",
       "      <td>52861.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            overall  unixReviewTime  helpful_ratings  total_ratings\n",
       "count  27477.000000    2.747700e+04     27477.000000   27477.000000\n",
       "mean       3.853769    1.261137e+09        47.941988      52.304982\n",
       "std        1.447439    9.132063e+07       339.326083     344.390836\n",
       "min        1.000000    9.572256e+08         0.000000      11.000000\n",
       "25%        3.000000    1.201651e+09        12.000000      14.000000\n",
       "50%        5.000000    1.282090e+09        19.000000      22.000000\n",
       "75%        5.000000    1.331251e+09        38.000000      43.000000\n",
       "max        5.000000    1.405642e+09     52176.000000   52861.000000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets checkout the extreme total ratings. It looks like an outlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>helpful</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>helpful_ratings</th>\n",
       "      <th>total_ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>396128</th>\n",
       "      <td>A1TTA1UUGY4WY4</td>\n",
       "      <td>B0047E0EII</td>\n",
       "      <td>SW3K</td>\n",
       "      <td>[52176, 52861]</td>\n",
       "      <td>For decades I have been trying to come up with...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>No more winning for you, Mr. Banana!</td>\n",
       "      <td>1299110400</td>\n",
       "      <td>03 3, 2011</td>\n",
       "      <td>52176</td>\n",
       "      <td>52861</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            reviewerID        asin reviewerName         helpful  \\\n",
       "396128  A1TTA1UUGY4WY4  B0047E0EII         SW3K  [52176, 52861]   \n",
       "\n",
       "                                               reviewText  overall  \\\n",
       "396128  For decades I have been trying to come up with...      5.0   \n",
       "\n",
       "                                     summary  unixReviewTime  reviewTime  \\\n",
       "396128  No more winning for you, Mr. Banana!      1299110400  03 3, 2011   \n",
       "\n",
       "        helpful_ratings  total_ratings  \n",
       "396128            52176          52861  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[dataset.helpful_ratings == dataset.helpful_ratings.max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.drop(dataset.index[dataset.helpful_ratings == dataset.helpful_ratings.max()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>helpful_ratings</th>\n",
       "      <th>total_ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>27476.000000</td>\n",
       "      <td>2.747600e+04</td>\n",
       "      <td>27476.000000</td>\n",
       "      <td>27476.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.853727</td>\n",
       "      <td>1.261136e+09</td>\n",
       "      <td>46.044766</td>\n",
       "      <td>50.382989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.447449</td>\n",
       "      <td>9.132200e+07</td>\n",
       "      <td>127.439043</td>\n",
       "      <td>130.782279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.572256e+08</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.201651e+09</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.282090e+09</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>22.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.331251e+09</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>43.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.405642e+09</td>\n",
       "      <td>6128.000000</td>\n",
       "      <td>6174.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            overall  unixReviewTime  helpful_ratings  total_ratings\n",
       "count  27476.000000    2.747600e+04     27476.000000   27476.000000\n",
       "mean       3.853727    1.261136e+09        46.044766      50.382989\n",
       "std        1.447449    9.132200e+07       127.439043     130.782279\n",
       "min        1.000000    9.572256e+08         0.000000      11.000000\n",
       "25%        3.000000    1.201651e+09        12.000000      14.000000\n",
       "50%        5.000000    1.282090e+09        19.000000      22.000000\n",
       "75%        5.000000    1.331251e+09        38.000000      43.000000\n",
       "max        5.000000    1.405642e+09      6128.000000    6174.000000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check the count of reviews greater than 1000 and remove them if they are making the distribution a bit more skewed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reviewerID         68\n",
       "asin               68\n",
       "reviewerName       68\n",
       "helpful            68\n",
       "reviewText         68\n",
       "overall            68\n",
       "summary            68\n",
       "unixReviewTime     68\n",
       "reviewTime         68\n",
       "helpful_ratings    68\n",
       "total_ratings      68\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[dataset.total_ratings > 1000].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The count is very less. Lets remove these and check the distribution again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.drop(dataset.index[dataset.total_ratings > 1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>helpful_ratings</th>\n",
       "      <th>total_ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>27408.000000</td>\n",
       "      <td>2.740800e+04</td>\n",
       "      <td>27408.000000</td>\n",
       "      <td>27408.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.852415</td>\n",
       "      <td>1.261193e+09</td>\n",
       "      <td>41.793126</td>\n",
       "      <td>46.024701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.447970</td>\n",
       "      <td>9.136917e+07</td>\n",
       "      <td>75.977905</td>\n",
       "      <td>78.878631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.572256e+08</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.201738e+09</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.282176e+09</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>22.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.331359e+09</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>42.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.405642e+09</td>\n",
       "      <td>976.000000</td>\n",
       "      <td>996.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            overall  unixReviewTime  helpful_ratings  total_ratings\n",
       "count  27408.000000    2.740800e+04     27408.000000   27408.000000\n",
       "mean       3.852415    1.261193e+09        41.793126      46.024701\n",
       "std        1.447970    9.136917e+07        75.977905      78.878631\n",
       "min        1.000000    9.572256e+08         0.000000      11.000000\n",
       "25%        3.000000    1.201738e+09        12.000000      14.000000\n",
       "50%        5.000000    1.282176e+09        19.000000      22.000000\n",
       "75%        5.000000    1.331359e+09        38.000000      42.000000\n",
       "max        5.000000    1.405642e+09       976.000000     996.000000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try 2 ways of calculating the optimal threshold.One with the normal dataset. Another applying the threshold function on  the svaled Dataset.Let's set aside another copy of the dataset which we will use for scaling the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_df = dataset.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "min_max_scaler = MinMaxScaler()\n",
    "\n",
    "def scaleColumns(df, cols_to_scale):\n",
    "    for col in cols_to_scale:\n",
    "        df[col] = pd.DataFrame(min_max_scaler.fit_transform(pd.DataFrame(scaled_df[col])),columns=[col])\n",
    "    return df\n",
    "scaled_df = scaleColumns(scaled_df,['helpful_ratings','total_ratings'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>helpful</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>helpful_ratings</th>\n",
       "      <th>total_ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A3UPYGJKZ0XTU4</td>\n",
       "      <td>0615391206</td>\n",
       "      <td>mirasreviews</td>\n",
       "      <td>[26, 27]</td>\n",
       "      <td>There is no shortage of pop recipes available ...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Excels at Sweet Dessert Pops, but Falls Short ...</td>\n",
       "      <td>1367712000</td>\n",
       "      <td>05 5, 2013</td>\n",
       "      <td>0.032787</td>\n",
       "      <td>0.028426</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewerID        asin  reviewerName   helpful  \\\n",
       "2  A3UPYGJKZ0XTU4  0615391206  mirasreviews  [26, 27]   \n",
       "\n",
       "                                          reviewText  overall  \\\n",
       "2  There is no shortage of pop recipes available ...      4.0   \n",
       "\n",
       "                                             summary  unixReviewTime  \\\n",
       "2  Excels at Sweet Dessert Pops, but Falls Short ...      1367712000   \n",
       "\n",
       "   reviewTime  helpful_ratings  total_ratings  \n",
       "2  05 5, 2013         0.032787       0.028426  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This funcion will get the helpfulness rating based on certain threshold and total and helpful ratings count\n",
    "def getHelpfulness(input_dataset, threshold):\n",
    "    threshold=threshold\n",
    "    input_dataset.loc[:, 'isHelpful'] = np.where(input_dataset['helpful_ratings']/input_dataset['total_ratings'] > threshold, 1, 0)\n",
    "    #dataset = dataset.drop(columns = ['helpful'], axis=1)\n",
    "    \n",
    "    return input_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOptimalThreshold(input_dataset):\n",
    "    \n",
    "    # Lets start with threshold value as 0.5 assuming the dataset is uniformly distributed\n",
    "    threshold = 0.5\n",
    "        \n",
    "    for x in range(1,6):\n",
    "        # Call getHelpfulness method an dcheck how the data is distributed in case of 0.5 threshold\n",
    "        print('Threshold : %f' %threshold)\n",
    "        input_dataset = getHelpfulness(input_dataset, threshold)\n",
    "        frequency = input_dataset['isHelpful'].value_counts()\n",
    "        print('Frequency of 0 value : %f' %frequency.loc[0])\n",
    "        print('Frequency of 1 value : %f' %frequency.loc[1])\n",
    "        if(frequency.loc[0]/(frequency[0]+frequency[1]) < 0.5):\n",
    "            threshold = threshold + 0.1\n",
    "        else:\n",
    "            threshold = threshold - 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold : 0.500000\n",
      "Frequency of 0 value : 1613.000000\n",
      "Frequency of 1 value : 25795.000000\n",
      "Threshold : 0.600000\n",
      "Frequency of 0 value : 2032.000000\n",
      "Frequency of 1 value : 25376.000000\n",
      "Threshold : 0.700000\n",
      "Frequency of 0 value : 2915.000000\n",
      "Frequency of 1 value : 24493.000000\n",
      "Threshold : 0.800000\n",
      "Frequency of 0 value : 4920.000000\n",
      "Frequency of 1 value : 22488.000000\n",
      "Threshold : 0.900000\n",
      "Frequency of 0 value : 10524.000000\n",
      "Frequency of 1 value : 16884.000000\n"
     ]
    }
   ],
   "source": [
    "getOptimalThreshold(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold : 0.500000\n",
      "Frequency of 0 value : 25274.000000\n",
      "Frequency of 1 value : 2134.000000\n",
      "Threshold : 0.400000\n",
      "Frequency of 0 value : 25267.000000\n",
      "Frequency of 1 value : 2141.000000\n",
      "Threshold : 0.300000\n",
      "Frequency of 0 value : 25260.000000\n",
      "Frequency of 1 value : 2148.000000\n",
      "Threshold : 0.200000\n",
      "Frequency of 0 value : 25251.000000\n",
      "Frequency of 1 value : 2157.000000\n",
      "Threshold : 0.100000\n",
      "Frequency of 0 value : 25240.000000\n",
      "Frequency of 1 value : 2168.000000\n"
     ]
    }
   ],
   "source": [
    "getOptimalThreshold(scaled_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling the dataframe doesnt help much. So lets go without scaling the columns value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the optimal threshold output. Let's select threshold as 0.8 and establish out target variable 'isHelpful' and drop the other 2 columns i.e 'helpful_ratings' and 'total_ratings'. I have selected 0.8 instead of 0.9 because i didnt wanted to select the extreme bound to avoid overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = getHelpfulness(dataset, threshold=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.drop(columns = ['helpful', 'helpful_ratings', 'total_ratings'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewTime</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>isHelpful</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5154</td>\n",
       "      <td>5154</td>\n",
       "      <td>5112</td>\n",
       "      <td>5154</td>\n",
       "      <td>5154</td>\n",
       "      <td>5154</td>\n",
       "      <td>5154</td>\n",
       "      <td>5154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22254</td>\n",
       "      <td>22254</td>\n",
       "      <td>22247</td>\n",
       "      <td>22254</td>\n",
       "      <td>22254</td>\n",
       "      <td>22254</td>\n",
       "      <td>22254</td>\n",
       "      <td>22254</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           reviewerID   asin  reviewerName  reviewText  overall  summary  \\\n",
       "isHelpful                                                                  \n",
       "0                5154   5154          5112        5154     5154     5154   \n",
       "1               22254  22254         22247       22254    22254    22254   \n",
       "\n",
       "           unixReviewTime  reviewTime  \n",
       "isHelpful                              \n",
       "0                    5154        5154  \n",
       "1                   22254       22254  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(dataset.groupby('isHelpful').count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x269fe22c0f0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.stats import norm, skew\n",
    "\n",
    "# Lets explore the distribution of the helpfulness target variable\n",
    "sns.distplot(dataset['isHelpful'], fit=norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Custom Feature Extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This utility will generate the difference between the first review and the corresponding review days difference.\n",
    "def getReviewTimeDifferenceFromMin(dataset):\n",
    "    dataset['reviewTime'] = pd.to_datetime(dataset['reviewTime'])\n",
    "    dataset['firstReviewTime'] = dataset.groupby(['asin'])['reviewTime'].transform(min)\n",
    "    dataset['review_first_diff'] = (dataset['reviewTime'] - dataset['firstReviewTime']).astype('timedelta64[D]')\n",
    "    dataset = dataset.drop(columns = ['firstReviewTime', 'reviewTime'])\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This utility will count the sentences in the review text\n",
    "def countReviewSentence(dataset):\n",
    "    pun_sen = ['.', '!', '?']\n",
    "    text_col = dataset['reviewText']\n",
    "    sentence_counts = []\n",
    "    for i in text_col:\n",
    "        sentence_count = []\n",
    "        for j in pun_sen:\n",
    "            count_a = i.count(j)\n",
    "            sentence_count.append(count_a)\n",
    "        sentence_counts.append(sum(sentence_count))\n",
    "    dataset['reviewSentencesCount'] = sentence_counts\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This utiity will generate the no of characters present in review text.\n",
    "# This will be used in determining the readability of each review\n",
    "punctuation = ['!','\"','#','$','%','&',\"'\",'(',')','*','+',',','-','.','/',':',';','<','=','>','?','@','[','\\\\',']','^','_','`','{','|','}','~','``',\"''\",'--']\n",
    "def count_characters(dataset):\n",
    "    reviewcharacters = []\n",
    "    text_col = dataset['reviewText']\n",
    "    for i in text_col:\n",
    "        a = dict(Counter(i))\n",
    "        b = {k:v for k, v in a.items() if k not in punctuation}\n",
    "        c = sum(list(b.values()))\n",
    "        reviewcharacters.append(c)\n",
    "    dataset['reviewChars'] = reviewcharacters\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Readability of each review (ARI as index to measure)\n",
    "def readability(dataset):\n",
    "    wordperSen = []\n",
    "    charperWord = []\n",
    "    reviewRead = []\n",
    "    len_df = len(dataset)\n",
    "    dataset['reviewTextWordCount'] = dataset['reviewText'].apply(lambda x: len(x.split()))\n",
    "    a = list(dataset['reviewTextWordCount'])\n",
    "    b = list(dataset['reviewSentencesCount'])\n",
    "    c = list(dataset['reviewChars'])\n",
    "    for i in range(len_df):\n",
    "        if b[i] == 0:\n",
    "            wordperSen.append(0)\n",
    "        else:\n",
    "            j = a[i] / b[i]\n",
    "            wordperSen.append(j)\n",
    "        if a[i] == 0:\n",
    "            charperWord.append(0)\n",
    "        else:\n",
    "            l = c[i] / a[i]\n",
    "            charperWord.append(l)\n",
    "        ari = 4.71 * charperWord[i] + 0.5 * wordperSen[i] - 21.43\n",
    "        reviewRead.append(ari)\n",
    "    dataset['reviewReadability'] = reviewRead\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop extra features in the dataset\n",
    "def drop_extra_features(dataset):\n",
    "    dataset = dataset.drop(columns = ['asin', 'reviewerName', 'reviewerID','summary','unixReviewTime', 'reviewTextWordCount','overall',  'reviewSentencesCount', 'reviewChars'], axis = 1)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will get all the customised features\n",
    "def getCustomFeatures(dataset):\n",
    "    start = time.time()\n",
    "    \n",
    "    dataset = getReviewTimeDifferenceFromMin(dataset)\n",
    "    dataset = countReviewSentence(dataset)\n",
    "    dataset = count_characters(dataset)\n",
    "    dataset = readability(dataset)\n",
    "    dataset = drop_extra_features(dataset)\n",
    "    \n",
    "    end = time.time()\n",
    "    print('Total time to derive the custom features {:4f} seconds'.format(end - start))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Defining Text Normalizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(x):\n",
    "\n",
    "    x = str(x)\n",
    "    for punct in \"/-'\":\n",
    "        x = x.replace(punct, ' ')\n",
    "    for punct in '&':\n",
    "        x = x.replace(punct, f' {punct} ')\n",
    "    for punct in '?!.,\"#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~' + '“”’':\n",
    "        x = x.replace(punct, '')\n",
    "    return x\n",
    "\n",
    "contraction_dict = {\"aint\": \"is not\", \"arent\": \"are not\",\"cant\": \"cannot\", \"'cause\": \"because\", \"couldve\": \"could have\", \"couldnt\": \"could not\", \"didnt\": \"did not\",  \"doesnt\": \"does not\", \"dont\": \"do not\", \"hadnt\": \"had not\", \"hasnt\": \"has not\", \"havent\": \"have not\", \"wasnt\": \"was not\", \"isnt\": \"is not\", \"shouldnt\": \"should not\", \"thats\": \"that is\"}\n",
    "def _get_contractions(contraction_dict):\n",
    "    contraction_re = re.compile('(%s)' % '|'.join(contraction_dict.keys()))\n",
    "    return contraction_dict, contraction_re\n",
    "\n",
    "contractions, contractions_re = _get_contractions(contraction_dict)\n",
    "\n",
    "def replace_contractions(text):\n",
    "    def replace(match):\n",
    "        return contractions[match.group(0)]\n",
    "    return contractions_re.sub(replace, text)\n",
    "\n",
    "# Misspelling corrections\n",
    "mispell_dict = {\"complis\": \"complies\", \"wellmade\": \"well-made\", \"onoff\": \"on/off\", \"kitchenaid\": \"kitchemaid\", \"roomba\": \"room\", \"appare\": \"apparel\", \"bodrum\": \"bodrum\", \"notly\": \"hotly\", \"kcup\": \"cup\", \"kcups\": \"cups\", \"krups\": \"cups\", \"roomthe\":\"room the\"}\n",
    "def _get_mispell(contraction_dict):\n",
    "    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n",
    "    return mispell_dict, mispell_re\n",
    "\n",
    "mispellings, mispellings_re = _get_mispell(mispell_dict)\n",
    "\n",
    "def replace_typical_mispell(text):\n",
    "    def replace(match):\n",
    "        return mispellings[match.group(0)]\n",
    "    return mispellings_re.sub(replace, text)\n",
    "\n",
    "wpt = nltk.WordPunctTokenizer()\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def normalize_text(text):\n",
    "    # lower case and remove special characters\\whitespaces\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text, re.I|re.A)\n",
    "    text = text.lower()\n",
    "    text = text.strip()\n",
    "    text = clean_text(text)\n",
    "    text = replace_contractions(text)\n",
    "    text = replace_typical_mispell(text)\n",
    "    # tokenize document\n",
    "    tokens = wpt.tokenize(text)\n",
    "    # filter stopwords out of document\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    # re-create document from filtered tokens\n",
    "    text = ' '.join(filtered_tokens)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def get_normalize_text_data(run='train'):\n",
    "def get_normalize_text_data(dataset):\n",
    "    start = time.time()\n",
    "    dataset['normalize_review_text'] = dataset['reviewText'].apply(lambda x: normalize_text(x))\n",
    "    dataset = dataset.drop(columns=['reviewText'])\n",
    "    end = time.time()\n",
    "    \n",
    "    print('Total time to normalize the text data {:4f} seconds'.format(end - start))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time to derive the custom features 10.323085 seconds\n",
      "Total time to normalize the text data 27.680471 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>isHelpful</th>\n",
       "      <th>review_first_diff</th>\n",
       "      <th>reviewReadability</th>\n",
       "      <th>normalize_review_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>640.0</td>\n",
       "      <td>12.653924</td>\n",
       "      <td>shortage pop recipes available free web purcha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   isHelpful  review_first_diff  reviewReadability  \\\n",
       "0          1              640.0          12.653924   \n",
       "\n",
       "                               normalize_review_text  \n",
       "0  shortage pop recipes available free web purcha...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate the custom features\n",
    "dataset = getCustomFeatures(dataset)\n",
    "# Generate the normalize text feature\n",
    "dataset = get_normalize_text_data(dataset)\n",
    "dataset.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train-Test Split Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 18363 entries, 8909 to 23654\n",
      "Data columns (total 3 columns):\n",
      "review_first_diff        18363 non-null float64\n",
      "reviewReadability        18363 non-null float64\n",
      "normalize_review_text    18363 non-null object\n",
      "dtypes: float64(2), object(1)\n",
      "memory usage: 573.8+ KB\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = dataset[['review_first_diff', 'reviewReadability', 'normalize_review_text']]\n",
    "y = dataset['isHelpful']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "X_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature Union and Function Transformer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "class ColumnExtractor(TransformerMixin):\n",
    "\n",
    "    def __init__(self, cols):\n",
    "        self.cols = cols\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # stateless transformer\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # assumes X is a DataFrame\n",
    "        Xcols = X[self.cols]\n",
    "        return Xcols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextTFIDFVectorizer(TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.vec = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # stateless transformer\n",
    "        self.vec =TfidfVectorizer(min_df=0.01, max_df=1., ngram_range=(1, 1))\n",
    "        self.vec.fit(X['normalize_review_text'])\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # assumes X is a DataFrame\n",
    "        # Defining the Tfidf vectorizer\n",
    "        #vectorizer = TfidfVectorizer(min_df=0.01, max_df=1., ngram_range=(1, 1))\n",
    "        #fit the vectorizers to the data.\n",
    "        features = self.vec.transform(X['normalize_review_text'])\n",
    "    \n",
    "        return features\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextWord2Vectorizer(TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.word2vec = None\n",
    "        \n",
    "    def _col_transform(self, words, model, vocabulary, num_features):\n",
    "        feature_vector = np.zeros((num_features,),dtype=\"float64\")\n",
    "        nwords = 0.\n",
    "    \n",
    "        for word in words:\n",
    "            if word in vocabulary: \n",
    "                nwords = nwords + 1.\n",
    "                feature_vector = np.add(feature_vector, model[word])\n",
    "\n",
    "        if nwords:\n",
    "            feature_vector = np.divide(feature_vector, nwords)\n",
    "\n",
    "        return feature_vector\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        wpt = nltk.WordPunctTokenizer()\n",
    "        tokenized_corpus = [wpt.tokenize(document) for document in X['normalize_review_text']] \n",
    "        \n",
    "        # Set values for various parameters\n",
    "        feature_size = 100    # Word vector dimensionality  \n",
    "        window_context = 5    # Context window size                                                                                    \n",
    "        min_word_count = 2    # Minimum word count                        \n",
    "        sample = 1e-3         # Downsample setting for frequent words\n",
    "\n",
    "        self.word2vec = Word2Vec(tokenized_corpus, size=feature_size, \n",
    "                                      window=window_context, min_count = min_word_count,\n",
    "                                      sample=sample, iter=10)\n",
    "        \n",
    "        vocabulary = set(self.word2vec.wv.index2word)\n",
    "        features = [self._col_transform(tokenized_sentence, self.word2vec, vocabulary, num_features=100)\n",
    "                    for tokenized_sentence in tokenized_corpus]\n",
    "        return np.array(features)\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "\n",
    "NUM_FEATURES = ['review_first_diff', 'reviewReadability']\n",
    "TEXT_FEATURE = ['normalize_review_text']\n",
    "\n",
    "process_and_join_features_tfidf = FeatureUnion(\n",
    "                            transformer_list=[\n",
    "                                ('numeric_feaures', Pipeline([\n",
    "                                    ('selector', ColumnExtractor(NUM_FEATURES)), \n",
    "                                    ('scale', StandardScaler())\n",
    "                                ])),\n",
    "                                ('text_features', Pipeline([\n",
    "                                    ('selector', ColumnExtractor(TEXT_FEATURE)),\n",
    "                                    ('vectorizer',TextTFIDFVectorizer())\n",
    "                                ]))\n",
    "                            ])\n",
    "\n",
    "\n",
    "process_and_join_features_word2vec = FeatureUnion(\n",
    "                            transformer_list=[\n",
    "                                ('numeric_feaures', Pipeline([\n",
    "                                    ('selector', ColumnExtractor(NUM_FEATURES)), \n",
    "                                    ('scale', StandardScaler())\n",
    "                                ])),\n",
    "                                ('text_features', Pipeline([\n",
    "                                    ('selector', ColumnExtractor(TEXT_FEATURE)),\n",
    "                                    ('vectorizer',TextWord2Vectorizer())\n",
    "                                ]))\n",
    "                            ])\n",
    "\n",
    "pipe_lr_tfidf = Pipeline([\n",
    "                   ('union', process_and_join_features_tfidf),\n",
    "                   ('clf', LogisticRegression(random_state=42))\n",
    "                ])\n",
    "pipe_dt_tfidf = Pipeline([\n",
    "                   ('union', process_and_join_features_tfidf),\n",
    "                   ('clf', DecisionTreeClassifier(random_state=42))\n",
    "                ])\n",
    "pipe_randomforest_tfidf = Pipeline([\n",
    "                   ('union', process_and_join_features_tfidf),\n",
    "                   ('clf', RandomForestClassifier(random_state=42))\n",
    "                ])\n",
    "pipe_adaboost_tfidf = Pipeline([\n",
    "                   ('union', process_and_join_features_tfidf),\n",
    "                   ('clf', AdaBoostClassifier(random_state=42))\n",
    "                ])\n",
    "\n",
    "pipe_lr_word2vec = Pipeline([\n",
    "                   ('union', process_and_join_features_word2vec),\n",
    "                   ('clf', LogisticRegression(random_state=42))\n",
    "                ])\n",
    "pipe_dt_word2vec = Pipeline([\n",
    "                   ('union', process_and_join_features_word2vec),\n",
    "                   ('clf', DecisionTreeClassifier(random_state=42))\n",
    "                ])\n",
    "pipe_randomforest_word2vec = Pipeline([\n",
    "                   ('union', process_and_join_features_word2vec),\n",
    "                   ('clf', RandomForestClassifier(random_state=42))\n",
    "                ])\n",
    "pipe_adaboost_word2vec = Pipeline([\n",
    "                   ('union', process_and_join_features_word2vec),\n",
    "                   ('clf', AdaBoostClassifier(random_state=42))\n",
    "                ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select Base Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def train_predict_pipeline(pipeline):\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred_train = pipeline.predict(X_train)\n",
    "    #Evaluate model\n",
    "    p_pred_test = pipeline.predict_proba(X_test)[:, 1]\n",
    "    auc_test = roc_auc_score(y_test, p_pred_test)\n",
    "    end = time.time()\n",
    "    print('Test set accuracy score: %.3f ' % auc_test)\n",
    "    \n",
    "clf_pipeline = [pipe_lr_tfidf, pipe_dt_tfidf, pipe_randomforest_tfidf, pipe_adaboost_tfidf, pipe_lr_word2vec, pipe_dt_word2vec,\n",
    "                pipe_randomforest_word2vec, pipe_adaboost_word2vec]\n",
    "clf_dict = {0: 'TF-IDF Logistic Regression', 1: 'TF-IDF Decision Tree', 2: 'TF-IDF Random Forest', 3: 'TF-IDF Adaboost', 4: 'Word2Vec Logistic Regression',\n",
    "             5: 'Word2Vec Decision Tree', 6: 'Word2Vec Random Forest', 7: 'Word2Vec Adaboost'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting test accuracy using : TF-IDF Logistic Regression\n",
      "Test set accuracy score: 0.748 \n",
      "Total time to train and predict : TF-IDF Logistic Regression is 11.997715 seconds\n",
      "Predicting test accuracy using : TF-IDF Decision Tree\n",
      "Test set accuracy score: 0.555 \n",
      "Total time to train and predict : TF-IDF Decision Tree is 38.306320 seconds\n",
      "Predicting test accuracy using : TF-IDF Random Forest\n",
      "Test set accuracy score: 0.704 \n",
      "Total time to train and predict : TF-IDF Random Forest is 70.805239 seconds\n",
      "Predicting test accuracy using : TF-IDF Adaboost\n",
      "Test set accuracy score: 0.702 \n",
      "Total time to train and predict : TF-IDF Adaboost is 25.498657 seconds\n",
      "Predicting test accuracy using : Word2Vec Logistic Regression\n",
      "Test set accuracy score: 0.675 \n",
      "Total time to train and predict : Word2Vec Logistic Regression is 139.980813 seconds\n",
      "Predicting test accuracy using : Word2Vec Decision Tree\n",
      "Test set accuracy score: 0.536 \n",
      "Total time to train and predict : Word2Vec Decision Tree is 148.810178 seconds\n",
      "Predicting test accuracy using : Word2Vec Random Forest\n",
      "Test set accuracy score: 0.672 \n",
      "Total time to train and predict : Word2Vec Random Forest is 177.597102 seconds\n",
      "Predicting test accuracy using : Word2Vec Adaboost\n",
      "Test set accuracy score: 0.655 \n",
      "Total time to train and predict : Word2Vec Adaboost is 160.290747 seconds\n"
     ]
    }
   ],
   "source": [
    "for idx, clf_pipe in enumerate(clf_pipeline):\n",
    "    print('Predicting test accuracy using : %s' % clf_dict[idx])\n",
    "    start = time.time()\n",
    "    train_predict_pipeline(clf_pipe)    \n",
    "    end = time.time()\n",
    "    \n",
    "    print('Total time to train and predict : {} is {:4f} seconds'.format(clf_dict[idx], (end-start)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above test accuracy test, TFIDF Logistic Regression has performed better than rest all models.\n",
    "So in the next step, we will try to optimize the test accuracy by Cross-validation using GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,precision_score,classification_report\n",
    "\n",
    "def perform_model_optimization(grids, grid_dict):\n",
    "    # Fit the grid search objects\n",
    "    print('Performing model optimizations...')\n",
    "    best_acc = 0.0\n",
    "    best_clf = 0\n",
    "    best_gs = ''\n",
    "    for idx, gs in enumerate(grids):\n",
    "        start = time.time()\n",
    "        print('\\nEstimator: %s' % grid_dict[idx])\n",
    "        # Fit grid search\n",
    "        gs.fit(X_train, y_train)\n",
    "        # Best params\n",
    "        print('Best params: %s' % gs.best_params_)\n",
    "        # Best training data accuracy\n",
    "        print('Best training accuracy: %.3f' % gs.best_score_)\n",
    "        # Predict on test data with best params\n",
    "        y_pred = gs.predict(X_test)\n",
    "        # Test data accuracy of model with best params\n",
    "        print('Test set precision score for best params: %.3f ' % precision_score(y_test, y_pred))\n",
    "        print('Classification report :')\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        # Track best (highest test accuracy) model\n",
    "        if precision_score(y_test, y_pred) > best_acc:\n",
    "            best_acc = precision_score(y_test, y_pred)\n",
    "            best_gs = gs\n",
    "            best_clf = idx\n",
    "        end= time.time()\n",
    "        print('Total time to perform model optimization {} : {:4f}'.format(grid_dict[idx], (end - start)))\n",
    "    print('\\nClassifier with best test set precision: %s' % grid_dict[best_clf])\n",
    "    return (best_clf,best_gs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Set grid search params\n",
    "param_range = [2, 4, 6, 8, 10]\n",
    "param_range_fl = [1.0, 0.5, 0.1]\n",
    "\n",
    "grid_params_lr = [{'clf__penalty': ['l1', 'l2'],\n",
    "        'clf__C': param_range_fl,\n",
    "        'clf__solver': ['liblinear']}]\n",
    "grid_params_dt=[{'clf__min_samples_split' : param_range[1:],\n",
    "                 'clf__max_depth': param_range}]\n",
    "\n",
    "grid_params_rf = [{'clf__criterion': ['gini', 'entropy'],\n",
    "        'clf__max_depth': param_range,\n",
    "        'clf__min_samples_split': param_range[1:]}]\n",
    "\n",
    "grid_params_adaboost = [{'clf__learning_rate': [0.01,0.05,0.1,0.3,1],\n",
    "        'clf__n_estimators': [50, 100]}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = -1\n",
    "\n",
    "gs_lr_tfidf = GridSearchCV(estimator=pipe_lr_tfidf,\n",
    "                     param_grid=grid_params_lr,\n",
    "                     scoring='precision',cv=5)\n",
    "\n",
    "gs_dt_tfidf = GridSearchCV(estimator=pipe_dt_tfidf,\n",
    "                    param_grid=grid_params_dt,\n",
    "                    scoring='precision',\n",
    "                    cv=5, n_jobs=jobs)\n",
    "gs_rf_tfidf = GridSearchCV(estimator=pipe_randomforest_tfidf,\n",
    "                    param_grid=grid_params_rf,\n",
    "                    scoring='precision',\n",
    "                    cv=5, n_jobs=jobs)\n",
    "gs_adaboost_tfidf = GridSearchCV(estimator=pipe_adaboost_tfidf,\n",
    "                    param_grid=grid_params_adaboost,\n",
    "                    scoring='precision',\n",
    "                    cv=5, n_jobs=jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of pipelines for ease of iteration\n",
    "grids = [gs_lr_tfidf, gs_dt_tfidf, gs_rf_tfidf, gs_adaboost_tfidf]\n",
    "\n",
    "# Dictionary of pipelines and classifier types for ease of reference\n",
    "grid_dict = {0: 'TF-IDF Logistic Regression', 1: 'TF-IDF Decision Tree', 2: 'TF-IDF RandomForest', 3: 'TF-IDF Adaboost'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing model optimizations...\n",
      "\n",
      "Estimator: TF-IDF Logistic Regression\n",
      "Best params: {'clf__C': 1.0, 'clf__penalty': 'l2', 'clf__solver': 'liblinear'}\n",
      "Best training accuracy: 0.829\n",
      "Test set precision score for best params: 0.834 \n",
      "Classification report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.16      0.25      1694\n",
      "           1       0.83      0.98      0.90      7351\n",
      "\n",
      "    accuracy                           0.82      9045\n",
      "   macro avg       0.72      0.57      0.57      9045\n",
      "weighted avg       0.79      0.82      0.78      9045\n",
      "\n",
      "Total time to perform model optimization TF-IDF Logistic Regression : 195.247039\n",
      "\n",
      "Estimator: TF-IDF Decision Tree\n",
      "Best params: {'clf__max_depth': 10, 'clf__min_samples_split': 4}\n",
      "Best training accuracy: 0.817\n",
      "Test set precision score for best params: 0.821 \n",
      "Classification report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.08      0.13      1694\n",
      "           1       0.82      0.97      0.89      7351\n",
      "\n",
      "    accuracy                           0.80      9045\n",
      "   macro avg       0.61      0.53      0.51      9045\n",
      "weighted avg       0.74      0.80      0.75      9045\n",
      "\n",
      "Total time to perform model optimization TF-IDF Decision Tree : 203.433977\n",
      "\n",
      "Estimator: TF-IDF RandomForest\n",
      "Best params: {'clf__criterion': 'gini', 'clf__max_depth': 2, 'clf__min_samples_split': 4}\n",
      "Best training accuracy: 0.812\n",
      "Test set precision score for best params: 0.813 \n",
      "Classification report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      1694\n",
      "           1       0.81      1.00      0.90      7351\n",
      "\n",
      "    accuracy                           0.81      9045\n",
      "   macro avg       0.41      0.50      0.45      9045\n",
      "weighted avg       0.66      0.81      0.73      9045\n",
      "\n",
      "Total time to perform model optimization TF-IDF RandomForest : 468.723692\n",
      "\n",
      "Estimator: TF-IDF Adaboost\n",
      "Best params: {'clf__learning_rate': 1, 'clf__n_estimators': 100}\n",
      "Best training accuracy: 0.830\n",
      "Test set precision score for best params: 0.831 \n",
      "Classification report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.15      0.23      1694\n",
      "           1       0.83      0.96      0.89      7351\n",
      "\n",
      "    accuracy                           0.81      9045\n",
      "   macro avg       0.66      0.56      0.56      9045\n",
      "weighted avg       0.77      0.81      0.77      9045\n",
      "\n",
      "Total time to perform model optimization TF-IDF Adaboost : 284.695117\n",
      "\n",
      "Classifier with best test set precision: TF-IDF Logistic Regression\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, GridSearchCV(cv=5, error_score=nan,\n",
       "              estimator=Pipeline(memory=None,\n",
       "                                 steps=[('union',\n",
       "                                         FeatureUnion(n_jobs=None,\n",
       "                                                      transformer_list=[('numeric_feaures',\n",
       "                                                                         Pipeline(memory=None,\n",
       "                                                                                  steps=[('selector',\n",
       "                                                                                          <__main__.ColumnExtractor object at 0x00000269D9136588>),\n",
       "                                                                                         ('scale',\n",
       "                                                                                          StandardScaler(copy=True,\n",
       "                                                                                                         with_mean=True,\n",
       "                                                                                                         with_std=True))],\n",
       "                                                                                  verbose=False)),\n",
       "                                                                        ('text_features',\n",
       "                                                                         Pipeline(memor...\n",
       "                                                            max_iter=100,\n",
       "                                                            multi_class='auto',\n",
       "                                                            n_jobs=None,\n",
       "                                                            penalty='l2',\n",
       "                                                            random_state=42,\n",
       "                                                            solver='lbfgs',\n",
       "                                                            tol=0.0001,\n",
       "                                                            verbose=0,\n",
       "                                                            warm_start=False))],\n",
       "                                 verbose=False),\n",
       "              iid='deprecated', n_jobs=None,\n",
       "              param_grid=[{'clf__C': [1.0, 0.5, 0.1],\n",
       "                           'clf__penalty': ['l1', 'l2'],\n",
       "                           'clf__solver': ['liblinear']}],\n",
       "              pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "              scoring='precision', verbose=0))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perform_model_optimization(grids, grid_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the classification report printed above, though we have high precision or accuracy of about 80%,still its not properly build as the ratio of helpfulness and not helpfulness data is skewed with high number of helpfulness label data  compared to not helpful data. In this case we will perform over-sampling of minority data(not helpful data) so that the data distribution is symmetric using SMOTE. We will select TF-IDF Logistic Regression and Adaboost Classifier as this 2 model performed quite good and percision is almost close to each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "pipe_lr_tfidf_random_sampling = Pipeline([\n",
    "                   ('union', process_and_join_features_tfidf),\n",
    "                   ('random_sampling', RandomOverSampler(random_state=777)),\n",
    "                   ('clf', LogisticRegression(random_state=42))\n",
    "                ])\n",
    "pipe_lr_tfidf_smote_sampling = Pipeline([\n",
    "                   ('union', process_and_join_features_tfidf),\n",
    "                   ('random_sampling', SMOTE(random_state=777)),\n",
    "                   ('clf', LogisticRegression(random_state=42))\n",
    "                ])\n",
    "pipe_adaboost_tfidf_random_sampling = Pipeline([\n",
    "                   ('union', process_and_join_features_tfidf),\n",
    "                   ('random_sampling', RandomOverSampler(random_state=777)),\n",
    "                   ('clf', AdaBoostClassifier(random_state=42))\n",
    "                ])\n",
    "pipe_adaboost_tfidf_smote_sampling = Pipeline([\n",
    "                   ('union', process_and_join_features_tfidf),\n",
    "                   ('random_sampling', SMOTE(random_state=777)),\n",
    "                   ('clf', AdaBoostClassifier(random_state=42))\n",
    "                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_lr_tfidf_random_sampling = GridSearchCV(estimator=pipe_lr_tfidf_random_sampling,\n",
    "                     param_grid=grid_params_lr,\n",
    "                     scoring='accuracy',cv=5)\n",
    "gs_lr_tfidf_smote_sampling = GridSearchCV(estimator=pipe_lr_tfidf_smote_sampling,\n",
    "                     param_grid=grid_params_lr,\n",
    "                     scoring='accuracy',cv=5)\n",
    "gs_adaboost_tfidf_random_sampling = GridSearchCV(estimator=pipe_adaboost_tfidf_random_sampling,\n",
    "                     param_grid=grid_params_adaboost,\n",
    "                     scoring='accuracy',cv=5)\n",
    "gs_adaboost_tfidf_smote_sampling = GridSearchCV(estimator=pipe_adaboost_tfidf_smote_sampling,\n",
    "                     param_grid=grid_params_adaboost,\n",
    "                     scoring='accuracy',cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of pipelines for ease of iteration\n",
    "grids_sampling = [gs_lr_tfidf_random_sampling, gs_lr_tfidf_smote_sampling, gs_adaboost_tfidf_random_sampling, gs_adaboost_tfidf_smote_sampling]\n",
    "\n",
    "# Dictionary of pipelines and classifier types for ease of reference\n",
    "grid_sampling_dict = {0: 'Random Oversampling TF-IDF Logistic Regression', 1: 'SMOTE Oversampling TF-IDF Logistic Regression',\n",
    "                     2: 'Random Oversampling TF-IDF Adaboost Classifier', 3: 'SMOTE Oversampling TF-IDF Adaboost Classifier'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing model optimizations...\n",
      "\n",
      "Estimator: Random Oversampling TF-IDF Logistic Regression\n",
      "Best params: {'clf__C': 0.5, 'clf__penalty': 'l2', 'clf__solver': 'liblinear'}\n",
      "Best training accuracy: 0.710\n",
      "Test set precision score for best params: 0.896 \n",
      "Classification report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.64      0.45      1694\n",
      "           1       0.90      0.72      0.80      7351\n",
      "\n",
      "    accuracy                           0.70      9045\n",
      "   macro avg       0.62      0.68      0.62      9045\n",
      "weighted avg       0.79      0.70      0.73      9045\n",
      "\n",
      "Total time to perform model optimization Random Oversampling TF-IDF Logistic Regression : 205.302248\n",
      "\n",
      "Estimator: SMOTE Oversampling TF-IDF Logistic Regression\n",
      "Best params: {'clf__C': 0.1, 'clf__penalty': 'l2', 'clf__solver': 'liblinear'}\n",
      "Best training accuracy: 0.706\n",
      "Test set precision score for best params: 0.901 \n",
      "Classification report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.35      0.66      0.46      1694\n",
      "           1       0.90      0.72      0.80      7351\n",
      "\n",
      "    accuracy                           0.71      9045\n",
      "   macro avg       0.63      0.69      0.63      9045\n",
      "weighted avg       0.80      0.71      0.74      9045\n",
      "\n",
      "Total time to perform model optimization SMOTE Oversampling TF-IDF Logistic Regression : 425.941943\n",
      "\n",
      "Estimator: Random Oversampling TF-IDF Adaboost Classifier\n",
      "Best params: {'clf__learning_rate': 1, 'clf__n_estimators': 100}\n",
      "Best training accuracy: 0.659\n",
      "Test set precision score for best params: 0.892 \n",
      "Classification report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.31      0.65      0.42      1694\n",
      "           1       0.89      0.66      0.76      7351\n",
      "\n",
      "    accuracy                           0.66      9045\n",
      "   macro avg       0.60      0.66      0.59      9045\n",
      "weighted avg       0.78      0.66      0.70      9045\n",
      "\n",
      "Total time to perform model optimization Random Oversampling TF-IDF Adaboost Classifier : 2068.855949\n",
      "\n",
      "Estimator: SMOTE Oversampling TF-IDF Adaboost Classifier\n",
      "Best params: {'clf__learning_rate': 1, 'clf__n_estimators': 100}\n",
      "Best training accuracy: 0.706\n",
      "Test set precision score for best params: 0.859 \n",
      "Classification report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.31      0.46      0.37      1694\n",
      "           1       0.86      0.76      0.81      7351\n",
      "\n",
      "    accuracy                           0.71      9045\n",
      "   macro avg       0.58      0.61      0.59      9045\n",
      "weighted avg       0.76      0.71      0.73      9045\n",
      "\n",
      "Total time to perform model optimization SMOTE Oversampling TF-IDF Adaboost Classifier : 1979.038631\n",
      "\n",
      "Classifier with best test set precision: SMOTE Oversampling TF-IDF Logistic Regression\n"
     ]
    }
   ],
   "source": [
    "best_clf,best_gs = perform_model_optimization(grids_sampling, grid_sampling_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved SMOTE Oversampling TF-IDF Logistic Regression grid search pipeline to file: best_gs_pipeline.pkl\n"
     ]
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "# Save best grid search pipeline to file\n",
    "dump_file = 'best_gs_pipeline.pkl'\n",
    "joblib.dump(best_gs, dump_file, compress=1)\n",
    "print('\\nSaved %s grid search pipeline to file: %s' % (grid_sampling_dict[best_clf], dump_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the our analysis, using SMOTE Oversampling TFIDF Logistic Regression, we got the highest precision of around 90%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
