{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Review Helpfulness Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we will try to automatically predict the helpfulness of Amazon.com product reviews, specifically in\n",
    "the case of home and kitchen products.\n",
    "\n",
    "The inspiration behind this project is that the product we buy is mostly influenced by the review about the product. In most of the e-commerce platform, the most helpful reviews are displayed on the products front page.In order to differentiate reviews based on their helpfulness, Amazon has implemented an interface that allows customers to vote on whether a particular review has been helpful or unhelpful. The fraction of customers who have deemed the review helpful is displayed with the review, and  amazon uses these ratings to rank the reviews, displaying the most helpful rankings on the products front page. The drawback is that more recently written reviews are at a disadvantage since less people have voted on the helpfulness of the review. Because of this, reviews with few votes cannot be effectively ranked and will not gain visibility until they have accumulated adequate votes, which can take some time.\n",
    "\n",
    "As a result, we would like to assess the helpfulness of reviews automatically, without having to wait for users to manually vote over the course of time. If we can do this, we would be able to give users the most relevant, helpful, and up to date reviews possible, without any delay in more helpful reviews being displayed. Moreover, such an automatic classification of reviews would be able to help in rooting out poorly written reviews lacking helpful information to other consumers.\n",
    "\n",
    "For the problem statement, we will use the Home and Kitchen dataset which is having around 346,355 reviews. Dataset is available at JmCauley page: http://jmcauley.ucsd.edu/data/amazon/links.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the relevant dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import math, time, random, datetime\n",
    "import string\n",
    "import os\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "from collections import defaultdict\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# data visualization\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "from IPython.display import display # Allows the use of display() for DataFrames\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import re\n",
    "from collections import Counter\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder where input data and pickle data load present\n",
    "data_folder = '../data/dataLoad/'\n",
    "input_folder = '../data/raw/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the home and kitchen dataset which is downloaded in the /data path\n",
    "input_data = 'reviews_Home_and_Kitchen_5.json.gz'\n",
    "def parse(path):\n",
    "    g = gzip.open(path, 'rb')\n",
    "    for l in g:\n",
    "        yield eval(l)\n",
    "\n",
    "def getDF(path):\n",
    "    i = 0\n",
    "    df = {}\n",
    "    for d in parse(path):\n",
    "        df[i] = d\n",
    "        i +=1\n",
    "    return pd.DataFrame.from_dict(df, orient='index')\n",
    "\n",
    "data = getDF(input_folder + input_data)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>helpful</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>APYOBQE6M18AA</td>\n",
       "      <td>0615391206</td>\n",
       "      <td>Martin Schwartz</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>My daughter wanted this book and the price on ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Best Price</td>\n",
       "      <td>1382140800</td>\n",
       "      <td>10 19, 2013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      reviewerID        asin     reviewerName helpful  \\\n",
       "0  APYOBQE6M18AA  0615391206  Martin Schwartz  [0, 0]   \n",
       "\n",
       "                                          reviewText  overall     summary  \\\n",
       "0  My daughter wanted this book and the price on ...      5.0  Best Price   \n",
       "\n",
       "   unixReviewTime   reviewTime  \n",
       "0      1382140800  10 19, 2013  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in the jmcauley site, belaw are the description of the fields:\n",
    "1. reviewerID - ID of the reviewer, e.g. A2SUAM1J3GNN3B\n",
    "2. asin - ID of the product, e.g. 0000013714\n",
    "3. reviewerName - name of the reviewer\n",
    "4. helpful - helpfulness rating of the review, e.g. 2/3.\n",
    "5. reviewText - text of the review\n",
    "6. overall - rating of the product\n",
    "7. summary - summary of the review\n",
    "8. unixReviewTime - time of the review (unix time)\n",
    "9. reviewTime - time of the review (raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before selecting features from the data, need to narrow down search to those reviews that had more than 2 total reviews. The reason we select 2 as threshold is because when total votes is 0 or 1, the help ratios turn out either 100% or 0%. Beyond that we also limited our search to reviews for products had at least 10 reviews, because automated helpfulness classification itself is not necessary if the given product for which the reviews are being displayed has few reviews to begin with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Filtering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(551682, 9)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Size of the data before filtering\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(462551, 9)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter the products which has atleast 10 reviews\n",
    "groupby_product = data.groupby('asin').size()\n",
    "filter_reviews = data.groupby('asin').size().values >= 10\n",
    "filter_asins = groupby_product[filter_reviews].index\n",
    "data_filter = data.loc[data['asin'].isin(filter_asins), :]\n",
    "data_filter.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27477, 9)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter the reviews having more than 10 helpful reviews\n",
    "dataset = data_filter[data_filter['helpful'].apply(lambda x: x[1] > 10)]\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our data is reduced down to 27477 product reviews. Next is splitting the data into training and testing set and feature selection where we select the fetaures or attributes needed for machine learning model. We will also build custom features which will help in predicting the helpfulness further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deriving Helpfulness Target variable from features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This funcion will get the helpfulness rating based on certain threshold and total and helpful ratings count\n",
    "def getHelpfulness(dataset):\n",
    "    threshold=0.1\n",
    "    dataset.loc[:, 'isHelpful'] = np.where(dataset['helpful'].apply(lambda x: x[0])/dataset['helpful'].apply(lambda x: x[1]) > threshold, 1, 0)\n",
    "    dataset = dataset.drop(columns = ['helpful'], axis=1)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "dataset = getHelpfulness(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1c43238e5c0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.stats import norm, skew\n",
    "\n",
    "# Lets explore the distribution of the helpfulness target variable\n",
    "sns.distplot(dataset['isHelpful'], fit=norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target variable distribution is not uniform. We will use SMOTE and random undersampling for balancing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Custom Feature Extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This utility will generate the difference between the first review and the corresponding review days difference.\n",
    "def getReviewTimeDifferenceFromMin(dataset):\n",
    "    dataset['reviewTime'] = pd.to_datetime(dataset['reviewTime'])\n",
    "    dataset['firstReviewTime'] = dataset.groupby(['asin'])['reviewTime'].transform(min)\n",
    "    dataset['review_first_diff'] = (dataset['reviewTime'] - dataset['firstReviewTime']).astype('timedelta64[D]')\n",
    "    dataset = dataset.drop(columns = ['firstReviewTime', 'reviewTime'])\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This utility will count the sentences in the review text\n",
    "def countReviewSentence(dataset):\n",
    "    pun_sen = ['.', '!', '?']\n",
    "    text_col = dataset['reviewText']\n",
    "    sentence_counts = []\n",
    "    for i in text_col:\n",
    "        sentence_count = []\n",
    "        for j in pun_sen:\n",
    "            count_a = i.count(j)\n",
    "            sentence_count.append(count_a)\n",
    "        sentence_counts.append(sum(sentence_count))\n",
    "    dataset['reviewSentencesCount'] = sentence_counts\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This utiity will generate the no of characters present in review text.\n",
    "# This will be used in determining the readability of each review\n",
    "punctuation = ['!','\"','#','$','%','&',\"'\",'(',')','*','+',',','-','.','/',':',';','<','=','>','?','@','[','\\\\',']','^','_','`','{','|','}','~','``',\"''\",'--']\n",
    "def count_characters(dataset):\n",
    "    reviewcharacters = []\n",
    "    text_col = dataset['reviewText']\n",
    "    for i in text_col:\n",
    "        a = dict(Counter(i))\n",
    "        b = {k:v for k, v in a.items() if k not in punctuation}\n",
    "        c = sum(list(b.values()))\n",
    "        reviewcharacters.append(c)\n",
    "    dataset['reviewChars'] = reviewcharacters\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Readability of each review (ARI as index to measure)\n",
    "def readability(dataset):\n",
    "    wordperSen = []\n",
    "    charperWord = []\n",
    "    reviewRead = []\n",
    "    len_df = len(dataset)\n",
    "    dataset['reviewTextWordCount'] = dataset['reviewText'].apply(lambda x: len(x.split()))\n",
    "    a = list(dataset['reviewTextWordCount'])\n",
    "    b = list(dataset['reviewSentencesCount'])\n",
    "    c = list(dataset['reviewChars'])\n",
    "    for i in range(len_df):\n",
    "        if b[i] == 0:\n",
    "            wordperSen.append(0)\n",
    "        else:\n",
    "            j = a[i] / b[i]\n",
    "            wordperSen.append(j)\n",
    "        if a[i] == 0:\n",
    "            charperWord.append(0)\n",
    "        else:\n",
    "            l = c[i] / a[i]\n",
    "            charperWord.append(l)\n",
    "        ari = 4.71 * charperWord[i] + 0.5 * wordperSen[i] - 21.43\n",
    "        reviewRead.append(ari)\n",
    "    dataset['reviewReadability'] = reviewRead\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop extra features in the dataset\n",
    "def drop_extra_features(dataset):\n",
    "    dataset = dataset.drop(columns = ['asin', 'reviewerName', 'reviewerID','summary','unixReviewTime', 'reviewTextWordCount','overall',  'reviewSentencesCount', 'reviewChars'], axis = 1)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will get all the customised features\n",
    "def getCustomFeatures(dataset):\n",
    "    \n",
    "    dataset = getReviewTimeDifferenceFromMin(dataset)\n",
    "    dataset = countReviewSentence(dataset)\n",
    "    dataset = count_characters(dataset)\n",
    "    dataset = readability(dataset)\n",
    "    dataset = drop_extra_features(dataset)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Defining Text Normalizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(x):\n",
    "\n",
    "    x = str(x)\n",
    "    for punct in \"/-'\":\n",
    "        x = x.replace(punct, ' ')\n",
    "    for punct in '&':\n",
    "        x = x.replace(punct, f' {punct} ')\n",
    "    for punct in '?!.,\"#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~' + '“”’':\n",
    "        x = x.replace(punct, '')\n",
    "    return x\n",
    "\n",
    "contraction_dict = {\"aint\": \"is not\", \"arent\": \"are not\",\"cant\": \"cannot\", \"'cause\": \"because\", \"couldve\": \"could have\", \"couldnt\": \"could not\", \"didnt\": \"did not\",  \"doesnt\": \"does not\", \"dont\": \"do not\", \"hadnt\": \"had not\", \"hasnt\": \"has not\", \"havent\": \"have not\", \"wasnt\": \"was not\", \"isnt\": \"is not\", \"shouldnt\": \"should not\", \"thats\": \"that is\"}\n",
    "def _get_contractions(contraction_dict):\n",
    "    contraction_re = re.compile('(%s)' % '|'.join(contraction_dict.keys()))\n",
    "    return contraction_dict, contraction_re\n",
    "\n",
    "contractions, contractions_re = _get_contractions(contraction_dict)\n",
    "\n",
    "def replace_contractions(text):\n",
    "    def replace(match):\n",
    "        return contractions[match.group(0)]\n",
    "    return contractions_re.sub(replace, text)\n",
    "\n",
    "# Misspelling corrections\n",
    "mispell_dict = {\"complis\": \"complies\", \"wellmade\": \"well-made\", \"onoff\": \"on/off\", \"kitchenaid\": \"kitchemaid\", \"roomba\": \"room\", \"appare\": \"apparel\", \"bodrum\": \"bodrum\", \"notly\": \"hotly\", \"kcup\": \"cup\", \"kcups\": \"cups\", \"krups\": \"cups\", \"roomthe\":\"room the\"}\n",
    "def _get_mispell(contraction_dict):\n",
    "    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n",
    "    return mispell_dict, mispell_re\n",
    "\n",
    "mispellings, mispellings_re = _get_mispell(mispell_dict)\n",
    "\n",
    "def replace_typical_mispell(text):\n",
    "    def replace(match):\n",
    "        return mispellings[match.group(0)]\n",
    "    return mispellings_re.sub(replace, text)\n",
    "\n",
    "wpt = nltk.WordPunctTokenizer()\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def normalize_text(text):\n",
    "    # lower case and remove special characters\\whitespaces\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text, re.I|re.A)\n",
    "    text = text.lower()\n",
    "    text = text.strip()\n",
    "    text = clean_text(text)\n",
    "    text = replace_contractions(text)\n",
    "    text = replace_typical_mispell(text)\n",
    "    # tokenize document\n",
    "    tokens = wpt.tokenize(text)\n",
    "    # filter stopwords out of document\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    # re-create document from filtered tokens\n",
    "    text = ' '.join(filtered_tokens)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def get_normalize_text_data(run='train'):\n",
    "def get_normalize_text_data(dataset):\n",
    "    dataset['normalize_review_text'] = dataset['reviewText'].apply(lambda x: normalize_text(x))\n",
    "    dataset = dataset.drop(columns=['reviewText'])\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>isHelpful</th>\n",
       "      <th>review_first_diff</th>\n",
       "      <th>reviewReadability</th>\n",
       "      <th>normalize_review_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>640.0</td>\n",
       "      <td>12.653924</td>\n",
       "      <td>shortage pop recipes available free web purcha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   isHelpful  review_first_diff  reviewReadability  \\\n",
       "0          1              640.0          12.653924   \n",
       "\n",
       "                               normalize_review_text  \n",
       "0  shortage pop recipes available free web purcha...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate the custom features\n",
    "dataset = getCustomFeatures(dataset)\n",
    "# Generate the normalize text feature\n",
    "dataset = get_normalize_text_data(dataset)\n",
    "dataset.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train-Test Split Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 18409 entries, 11206 to 23654\n",
      "Data columns (total 3 columns):\n",
      "review_first_diff        18409 non-null float64\n",
      "reviewReadability        18409 non-null float64\n",
      "normalize_review_text    18409 non-null object\n",
      "dtypes: float64(2), object(1)\n",
      "memory usage: 575.3+ KB\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = dataset[['review_first_diff', 'reviewReadability', 'normalize_review_text']]\n",
    "y = dataset['isHelpful']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from imblearn.over_sampling import SMOTENC\n",
    "# define pipeline\n",
    "#over = SMOTENC(sampling_strategy=0.1)\n",
    "#under = RandomUnderSampler(sampling_strategy=0.5)\n",
    "#steps = [('o', over), ('u', under)]\n",
    "#pipeline = Pipeline(steps=steps)\n",
    "\n",
    "#print(\"Before OverSampling, counts of label '1': {}\".format(sum(y == 1))) \n",
    "#print(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y == 0))) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the dataset\n",
    "#X_list = np.array(list(X)).reshape(-1, 1)\n",
    "#X_res, y_res = pipeline.fit_resample(X_list, y)\n",
    "\n",
    "#print(\"After OverSampling, counts of label '1': {}\".format(sum(y_train == 1))) \n",
    "#print(\"After OverSampling, counts of label '0': {} \\n\".format(sum(y_train == 0))) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature Union and Function Transformer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "class ColumnExtractor(TransformerMixin):\n",
    "\n",
    "    def __init__(self, cols):\n",
    "        self.cols = cols\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # stateless transformer\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # assumes X is a DataFrame\n",
    "        Xcols = X[self.cols]\n",
    "        return Xcols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextTFIDFVectorizer(TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.vec = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # stateless transformer\n",
    "        self.vec =TfidfVectorizer(min_df=0.01, max_df=1., ngram_range=(1, 1))\n",
    "        self.vec.fit(X['normalize_review_text'])\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # assumes X is a DataFrame\n",
    "        # Defining the Tfidf vectorizer\n",
    "        #vectorizer = TfidfVectorizer(min_df=0.01, max_df=1., ngram_range=(1, 1))\n",
    "        #fit the vectorizers to the data.\n",
    "        features = self.vec.transform(X['normalize_review_text'])\n",
    "    \n",
    "        return features\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextWord2Vectorizer(TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.word2vec = None\n",
    "        \n",
    "    def _col_transform(self, words, model, vocabulary, num_features):\n",
    "        feature_vector = np.zeros((num_features,),dtype=\"float64\")\n",
    "        nwords = 0.\n",
    "    \n",
    "        for word in words:\n",
    "            if word in vocabulary: \n",
    "                nwords = nwords + 1.\n",
    "                feature_vector = np.add(feature_vector, model[word])\n",
    "\n",
    "        if nwords:\n",
    "            feature_vector = np.divide(feature_vector, nwords)\n",
    "\n",
    "        return feature_vector\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        wpt = nltk.WordPunctTokenizer()\n",
    "        tokenized_corpus = [wpt.tokenize(document) for document in X['normalize_review_text']] \n",
    "        \n",
    "        # Set values for various parameters\n",
    "        feature_size = 100    # Word vector dimensionality  \n",
    "        window_context = 5    # Context window size                                                                                    \n",
    "        min_word_count = 2    # Minimum word count                        \n",
    "        sample = 1e-3         # Downsample setting for frequent words\n",
    "\n",
    "        self.word2vec = Word2Vec(tokenized_corpus, size=feature_size, \n",
    "                                      window=window_context, min_count = min_word_count,\n",
    "                                      sample=sample, iter=10)\n",
    "        \n",
    "        vocabulary = set(self.word2vec.wv.index2word)\n",
    "        features = [self._col_transform(tokenized_sentence, self.word2vec, vocabulary, num_features=100)\n",
    "                    for tokenized_sentence in tokenized_corpus]\n",
    "        return np.array(features)\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "NUM_FEATURES = ['review_first_diff', 'reviewReadability']\n",
    "TEXT_FEATURE = ['normalize_review_text']\n",
    "\n",
    "process_and_join_features_tfidf = FeatureUnion(\n",
    "                            transformer_list=[\n",
    "                                ('numeric_feaures', Pipeline([\n",
    "                                    ('selector', ColumnExtractor(NUM_FEATURES)), \n",
    "                                    ('scale', StandardScaler())\n",
    "                                ])),\n",
    "                                ('text_features', Pipeline([\n",
    "                                    ('selector', ColumnExtractor(TEXT_FEATURE)),\n",
    "                                    ('vectorizer',TextTFIDFVectorizer())\n",
    "                                ]))\n",
    "                            ])\n",
    "\n",
    "\n",
    "process_and_join_features_word2vec = FeatureUnion(\n",
    "                            transformer_list=[\n",
    "                                ('numeric_feaures', Pipeline([\n",
    "                                    ('selector', ColumnExtractor(NUM_FEATURES)), \n",
    "                                    ('scale', StandardScaler())\n",
    "                                ])),\n",
    "                                ('text_features', Pipeline([\n",
    "                                    ('selector', ColumnExtractor(TEXT_FEATURE)),\n",
    "                                    ('vectorizer',TextWord2Vectorizer())\n",
    "                                ]))\n",
    "                            ])\n",
    "\n",
    "pipe_lr_tfidf = Pipeline([\n",
    "                   ('union', process_and_join_features_tfidf),\n",
    "                   ('clf', LogisticRegression(random_state=42))\n",
    "                ])\n",
    "pipe_dt_tfidf = Pipeline([\n",
    "                   ('union', process_and_join_features_tfidf),\n",
    "                   ('clf', DecisionTreeClassifier(random_state=42))\n",
    "                ])\n",
    "pipe_lr_word2vec = Pipeline([\n",
    "                   ('union', process_and_join_features_word2vec),\n",
    "                   ('clf', LogisticRegression(random_state=42))\n",
    "                ])\n",
    "pipe_dt_word2vec = Pipeline([\n",
    "                   ('union', process_and_join_features_word2vec),\n",
    "                   ('clf', DecisionTreeClassifier(random_state=42))\n",
    "                ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select Base Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def train_predict_pipeline(pipeline):\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred_train = pipeline.predict(X_train)\n",
    "    #Evaluate model\n",
    "    p_pred_test = pipeline.predict_proba(X_test)[:, 1]\n",
    "    auc_test = roc_auc_score(y_test, p_pred_test)\n",
    "    print('Test set accuracy score: %.3f ' % auc_test)\n",
    "    \n",
    "clf_pipeline = [pipe_lr_tfidf, pipe_dt_tfidf, pipe_lr_word2vec, pipe_dt_word2vec]\n",
    "clf_dict = {0: 'TF-IDF Logistic Regression', 1: 'TF-IDF Decision Tree', 2: 'Word2Vec Logistic Regression',\n",
    "             3: 'Word2Vec Decision Tree'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting test accuracy using : TF-IDF Logistic Regression\n",
      "Test set accuracy score: 0.841 \n",
      "Predicting test accuracy using : TF-IDF Decision Tree\n",
      "Test set accuracy score: 0.556 \n",
      "Predicting test accuracy using : Word2Vec Logistic Regression\n",
      "Test set accuracy score: 0.760 \n",
      "Predicting test accuracy using : Word2Vec Decision Tree\n",
      "Test set accuracy score: 0.518 \n"
     ]
    }
   ],
   "source": [
    "for idx, clf_pipe in enumerate(clf_pipeline):\n",
    "    print('Predicting test accuracy using : %s' % clf_dict[idx])\n",
    "    train_predict_pipeline(clf_pipe)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above test accuracy test, TFIDF Logistic Regression has performed better than rest all models.\n",
    "So in the next step, we will try to optimize the test accuracy by Cross-validation using GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,precision_score,classification_report\n",
    "\n",
    "def perform_model_optimization(grids, grid_dict):\n",
    "    # Fit the grid search objects\n",
    "    print('Performing model optimizations...')\n",
    "    best_acc = 0.0\n",
    "    best_clf = 0\n",
    "    best_gs = ''\n",
    "    for idx, gs in enumerate(grids):\n",
    "        print('\\nEstimator: %s' % grid_dict[idx])\n",
    "        # Fit grid search\n",
    "        gs.fit(X_train, y_train)\n",
    "        # Best params\n",
    "        print('Best params: %s' % gs.best_params_)\n",
    "        # Best training data accuracy\n",
    "        print('Best training accuracy: %.3f' % gs.best_score_)\n",
    "        # Predict on test data with best params\n",
    "        y_pred = gs.predict(X_test)\n",
    "        # Test data accuracy of model with best params\n",
    "        print('Test set precision score for best params: %.3f ' % precision_score(y_test, y_pred))\n",
    "        print('Classification report :')\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        # Track best (highest test accuracy) model\n",
    "        if precision_score(y_test, y_pred) > best_acc:\n",
    "            best_acc = precision_score(y_test, y_pred)\n",
    "            best_gs = gs\n",
    "            best_clf = idx\n",
    "    print('\\nClassifier with best test set precision: %s' % grid_dict[best_clf])\n",
    "    return (best_clf,best_gs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Set grid search params\n",
    "param_range_fl = [1.0, 0.5, 0.1]\n",
    "\n",
    "grid_params_lr = [{'clf__penalty': ['l1', 'l2'],\n",
    "        'clf__C': param_range_fl,\n",
    "        'clf__solver': ['liblinear']}] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_lr_tfidf = GridSearchCV(estimator=pipe_lr_tfidf,\n",
    "                     param_grid=grid_params_lr,\n",
    "                     scoring='accuracy',cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of pipelines for ease of iteration\n",
    "grids = [gs_lr_tfidf]\n",
    "\n",
    "# Dictionary of pipelines and classifier types for ease of reference\n",
    "grid_dict = {0: 'TF-IDF Logistic Regression'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing model optimizations...\n",
      "\n",
      "Estimator: TF-IDF Logistic Regression\n",
      "Best params: {'clf__C': 1.0, 'clf__penalty': 'l1', 'clf__solver': 'liblinear'}\n",
      "Best training accuracy: 0.987\n",
      "Test set precision score for best params: 0.988 \n",
      "Classification report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.02      0.04       107\n",
      "           1       0.99      1.00      0.99      8961\n",
      "\n",
      "    accuracy                           0.99      9068\n",
      "   macro avg       0.99      0.51      0.52      9068\n",
      "weighted avg       0.99      0.99      0.98      9068\n",
      "\n",
      "\n",
      "Classifier with best test set precision: TF-IDF Logistic Regression\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, GridSearchCV(cv=5, error_score=nan,\n",
       "              estimator=Pipeline(memory=None,\n",
       "                                 steps=[('union',\n",
       "                                         FeatureUnion(n_jobs=None,\n",
       "                                                      transformer_list=[('numeric_feaures',\n",
       "                                                                         Pipeline(memory=None,\n",
       "                                                                                  steps=[('selector',\n",
       "                                                                                          <__main__.ColumnExtractor object at 0x000001C40AE8ADD8>),\n",
       "                                                                                         ('scale',\n",
       "                                                                                          StandardScaler(copy=True,\n",
       "                                                                                                         with_mean=True,\n",
       "                                                                                                         with_std=True))],\n",
       "                                                                                  verbose=False)),\n",
       "                                                                        ('text_features',\n",
       "                                                                         Pipeline(memor...\n",
       "                                                            max_iter=100,\n",
       "                                                            multi_class='auto',\n",
       "                                                            n_jobs=None,\n",
       "                                                            penalty='l2',\n",
       "                                                            random_state=42,\n",
       "                                                            solver='lbfgs',\n",
       "                                                            tol=0.0001,\n",
       "                                                            verbose=0,\n",
       "                                                            warm_start=False))],\n",
       "                                 verbose=False),\n",
       "              iid='deprecated', n_jobs=None,\n",
       "              param_grid=[{'clf__C': [1.0, 0.5, 0.1],\n",
       "                           'clf__penalty': ['l1', 'l2'],\n",
       "                           'clf__solver': ['liblinear']}],\n",
       "              pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "              scoring='accuracy', verbose=0))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perform_model_optimization(grids, grid_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the classification report printed above, though we have high precision or accuracy of about 99%, still its not properly build as the ratio of helpfulness and not helpfulness data is highly skewed with high number of helpfulness label data  compared to not helpful data. In this case we will perform over-sampling of minority data(not helpful data) so that the data distribution is symmetric using SMOTE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "pipe_lr_tfidf_random_sampling = Pipeline([\n",
    "                   ('union', process_and_join_features_tfidf),\n",
    "                   ('random_sampling', RandomOverSampler(random_state=777)),\n",
    "                   ('clf', LogisticRegression(random_state=42))\n",
    "                ])\n",
    "pipe_lr_tfidf_smote_sampling = Pipeline([\n",
    "                   ('union', process_and_join_features_tfidf),\n",
    "                   ('random_sampling', SMOTE(random_state=777)),\n",
    "                   ('clf', LogisticRegression(random_state=42))\n",
    "                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_lr_tfidf_random_sampling = GridSearchCV(estimator=pipe_lr_tfidf_random_sampling,\n",
    "                     param_grid=grid_params_lr,\n",
    "                     scoring='accuracy',cv=5)\n",
    "gs_lr_tfidf_smote_sampling = GridSearchCV(estimator=pipe_lr_tfidf_smote_sampling,\n",
    "                     param_grid=grid_params_lr,\n",
    "                     scoring='accuracy',cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of pipelines for ease of iteration\n",
    "grids_sampling = [gs_lr_tfidf_random_sampling, gs_lr_tfidf_smote_sampling]\n",
    "\n",
    "# Dictionary of pipelines and classifier types for ease of reference\n",
    "grid_sampling_dict = {0: 'Random Oversampling TF-IDF Logistic Regression', 1: 'SMOTE Oversampling TF-IDF Logistic Regression'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing model optimizations...\n",
      "\n",
      "Estimator: Random Oversampling TF-IDF Logistic Regression\n",
      "Best params: {'clf__C': 1.0, 'clf__penalty': 'l1', 'clf__solver': 'liblinear'}\n",
      "Best training accuracy: 0.956\n",
      "Test set precision score for best params: 0.992 \n",
      "Classification report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.10      0.38      0.16       107\n",
      "           1       0.99      0.96      0.98      8961\n",
      "\n",
      "    accuracy                           0.95      9068\n",
      "   macro avg       0.55      0.67      0.57      9068\n",
      "weighted avg       0.98      0.95      0.97      9068\n",
      "\n",
      "\n",
      "Estimator: SMOTE Oversampling TF-IDF Logistic Regression\n",
      "Best params: {'clf__C': 1.0, 'clf__penalty': 'l1', 'clf__solver': 'liblinear'}\n",
      "Best training accuracy: 0.956\n",
      "Test set precision score for best params: 0.993 \n",
      "Classification report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.11      0.39      0.17       107\n",
      "           1       0.99      0.96      0.98      8961\n",
      "\n",
      "    accuracy                           0.95      9068\n",
      "   macro avg       0.55      0.68      0.57      9068\n",
      "weighted avg       0.98      0.95      0.97      9068\n",
      "\n",
      "\n",
      "Classifier with best test set precision: SMOTE Oversampling TF-IDF Logistic Regression\n"
     ]
    }
   ],
   "source": [
    "best_clf,best_gs = perform_model_optimization(grids_sampling, grid_sampling_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved SMOTE Oversampling TF-IDF Logistic Regression grid search pipeline to file: best_gs_pipeline.pkl\n"
     ]
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "# Save best grid search pipeline to file\n",
    "dump_file = 'best_gs_pipeline.pkl'\n",
    "joblib.dump(best_gs, dump_file, compress=1)\n",
    "print('\\nSaved %s grid search pipeline to file: %s' % (grid_sampling_dict[best_clf], dump_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the our analysis, using SMOTE Oversampling TFIDF Logistic Regression, we got the highest precision of around 99%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
